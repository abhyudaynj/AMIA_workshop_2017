{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Available devices ['/cpu:0', '/gpu:0']\n"
     ]
    }
   ],
   "source": [
    "%reset -f\n",
    "import tensorflow as tf\n",
    "sess = tf.Session()\n",
    "import keras\n",
    "from keras import backend as K\n",
    "K.set_session(sess)\n",
    "\n",
    "from tensorflow.python.client import device_lib\n",
    "print('Available devices',[dvc.name for dvc in device_lib.list_local_devices()])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocess the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample sentence \n",
      " [7641, 3026, 1943, 8199, 11619, 5306, 3595, 972, 4611, 1, 1, 1211, 1, 7267, 5499, 7626, 4375, 1122, 3595, 8534, 6681, 6775, 4034, 16122, 464, 12445, 1420, 4375, 3595, 2888, 2101, 8244, 6681, 13281, 2789, 3791, 1, 13189, 3026, 15585, 5306, 1, 1211, 15271, 4611]\n",
      "Sample output \n",
      " [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 2, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "X shape (40163, 50)\n",
      "Y shape (40163, 50, 3)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import utils as data_utils\n",
    "X,Y=data_utils.read_dataset('processed.csv')\n",
    "X_vocab,Y_vocab=data_utils.get_vocab(X,Y)\n",
    "\n",
    "# Invert the vocabulary dict for output mapping\n",
    "X_inv={idx:word_string for word_string,idx in X_vocab.items()}\n",
    "Y_inv={idx:word_string for word_string,idx in Y_vocab.items()}\n",
    "newX,newY=data_utils.encode_dataset(X,Y,X_vocab,Y_vocab)\n",
    "\n",
    "print('Sample sentence \\n',newX[0])\n",
    "print('Sample output \\n',newY[0])\n",
    "padded_X=keras.preprocessing.sequence.pad_sequences(newX,maxlen=50)\n",
    "padded_Y=keras.preprocessing.sequence.pad_sequences(newY,maxlen=50)\n",
    "padded_Y=np.identity(len(Y_vocab))[padded_Y]\n",
    "print('X shape',padded_X.shape)\n",
    "print('Y shape',padded_Y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare Train and Test Splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X shape (36144, 50)\n",
      "Y shape (36144, 50, 3)\n",
      "[    0     0     0     0     0     0     0     0     0     0     0  2264\n",
      "  3026 10407 11974  7315  6572  9112  3595 15452  6681 14643  4375  7007\n",
      "  8686  2981 15415  7276  1420  6350 13677 11829  2981   266 11785  9671\n",
      "   549  1420  2981    12  9385  2981     1  9112  6775  2139 10422  6681\n",
      "  7974  4611]\n"
     ]
    }
   ],
   "source": [
    "# Shuffle the dataset order\n",
    "from sklearn.utils import shuffle\n",
    "padded_X,padded_Y= shuffle(padded_X,padded_Y)\n",
    "\n",
    "# Use 10% of the dataset for test samples\n",
    "datasize=padded_X.shape[0]\n",
    "trainsize=(datasize//10)*9\n",
    "\n",
    "#Split the dataset\n",
    "X_train,Y_train=padded_X[:trainsize],padded_Y[:trainsize]\n",
    "X_test,Y_test=padded_X[trainsize:],padded_Y[trainsize:]\n",
    "print('X shape',X_train.shape)\n",
    "print('Y shape',Y_train.shape)\n",
    "print(X_train[10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define a Single Layer RNN Sequence Labeler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, None, 200)         3259400   \n",
      "_________________________________________________________________\n",
      "lstm_1 (LSTM)                (None, None, 100)         120400    \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, None, 100)         0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_1 (Batch (None, None, 100)         400       \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, None, 100)         0         \n",
      "_________________________________________________________________\n",
      "time_distributed_1 (TimeDist (None, None, 3)           303       \n",
      "=================================================================\n",
      "Total params: 3,380,503\n",
      "Trainable params: 3,380,303\n",
      "Non-trainable params: 200\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "#Import relevant modules from Keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import TimeDistributed,Bidirectional\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Dropout\n",
    "from keras.layers import Embedding\n",
    "from keras.layers import LSTM,GRU,Activation\n",
    "\n",
    "#Define the model\n",
    "model=Sequential()\n",
    "model.add(Embedding(len(X_vocab),200,mask_zero=True))\n",
    "model.add(LSTM(100,return_sequences=True)) # Can be changed to Bidirectional(LSTM())\n",
    "model.add(Activation('relu')) \n",
    "model.add(keras.layers.BatchNormalization())\n",
    "model.add(Dropout(0.25))\n",
    "model.add(TimeDistributed(Dense(len(Y_vocab),activation='softmax')))\n",
    "adagrad_optimizer=keras.optimizers.Adagrad(lr=0.1)\n",
    "model.compile(loss='categorical_crossentropy',optimizer=adagrad_optimizer,metrics=['accuracy'])\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Start the Training\n",
    "One epoch is one entire dataset iteration or (Dataset Size)/(batch size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 32529 samples, validate on 3615 samples\n",
      "Epoch 1/1\n",
      "38s - loss: 0.2827 - acc: 0.8966 - val_loss: 0.3097 - val_acc: 0.8985\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f9c3ec44a58>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X_train,Y_train,epochs=1,batch_size=128,verbose=2,validation_split=0.1)\n",
    "\n",
    "# Early Stopping\n",
    "#mycallback=keras.callbacks.EarlyStopping(monitor='val_loss', patience =3)\n",
    "#model.fit(X_train,Y_train,epochs=10,batch_size=128,verbose=2,validation_split=0.1,callbacks =[mycallback])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4019/4019 [==============================] - 9s     \n",
      "Accuracy 0.896558438376\n"
     ]
    }
   ],
   "source": [
    "score,accuracy=model.evaluate(X_test,Y_test)\n",
    "print(\"Accuracy\",accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Examine predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "outp=model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('__MASK__', 'clnd'), ('__MASK__', 'clnd'), ('__MASK__', 'clnd'), ('__MASK__', 'clnd'), ('__MASK__', 'clnd'), ('__MASK__', 'clnd'), ('__MASK__', 'clnd'), ('__MASK__', 'clnd'), ('__MASK__', 'clnd'), ('__MASK__', 'clnd'), ('__MASK__', 'clnd'), ('__MASK__', 'clnd'), ('__MASK__', 'clnd'), ('__MASK__', 'clnd'), ('__MASK__', 'clnd'), ('__MASK__', 'clnd'), ('__MASK__', 'clnd'), ('__MASK__', 'clnd'), ('__MASK__', 'clnd'), ('__MASK__', 'clnd'), ('__MASK__', 'clnd'), ('__MASK__', 'clnd'), ('__MASK__', 'clnd'), ('__MASK__', 'clnd'), ('__MASK__', 'clnd'), ('__MASK__', 'clnd'), ('__MASK__', 'clnd'), ('__MASK__', 'clnd'), ('__MASK__', 'clnd'), ('__MASK__', 'clnd'), ('__MASK__', 'clnd'), ('__MASK__', 'clnd'), ('__MASK__', 'clnd'), ('__MASK__', 'clnd'), ('__MASK__', 'clnd'), ('__MASK__', 'clnd'), ('__MASK__', 'clnd'), ('__MASK__', 'clnd'), ('In', 'Outside'), ('brachial', 'Outside'), ('plexus', 'dsyn'), ('neuritis', 'dsyn'), (',', 'Outside'), ('conservative', 'Outside'), ('management', 'Outside'), ('may', 'Outside'), ('be', 'Outside'), ('more', 'Outside'), ('appropriate', 'Outside'), ('.', 'Outside')]\n"
     ]
    }
   ],
   "source": [
    "def get_predictions(x,y,X_inv,Y_inv):\n",
    "    input_words=[X_inv[word] for word in x]\n",
    "    output_labels=np.argmax(outp[1],axis=1)\n",
    "    output_labels=[Y_inv[label] for label in output_labels.tolist()]\n",
    "    print(list(zip(input_words,output_labels)))\n",
    "    \n",
    "get_predictions(X_test[1],outp[1],X_inv,Y_inv)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CNN-LSTM Model definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Conv1D\n",
    "model=Sequential()\n",
    "model.add(Embedding(len(X_vocab),200)) # Cannot use Mask because of CNN layer. An explicit mask can be passed to one of the later layers.\n",
    "model.add(Conv1D(100,5,strides=1,padding='same'))\n",
    "model.add(Activation('relu'))\n",
    "model.add(GRU(50,return_sequences=True))\n",
    "model.add(Activation('relu'))\n",
    "model.add(keras.layers.BatchNormalization())\n",
    "model.add(Dropout(0.25))\n",
    "model.add(TimeDistributed(Dense(len(Y_vocab),activation='softmax')))\n",
    "adagrad_optimizer=keras.optimizers.Adagrad(lr=0.1)\n",
    "model.compile(loss='categorical_crossentropy',optimizer=adagrad_optimizer,metrics=['accuracy'])\n",
    "print(model.summary())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
